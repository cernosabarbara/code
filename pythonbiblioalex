import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import networkx as nx
from collections import Counter

#Ensure NLTK resources are available
nltk.download('punkt')
nltk.download('stopwords')

#Define search query for OpenAlex API
query = "artificial intelligence surveillance workplace"
base_url = f"https://api.openalex.org/works?filter=title.search:{query},publication_year:2015-2024"

#Fetch data from OpenAlex API
response = requests.get(base_url)
data = response.json()


#kEYWORD SEARCH
import requests
import pandas as pd
from collections import Counter
from tqdm import tqdm

# Define your keywords
keywords = [
    "artificial intelligence", "workplace surveillance", "algorithmic management",
    "employee monitoring", "bossware", "digital monitoring", "productivity tracking",
    "workplace privacy", "surveillance ethics", "algorithmic bias", "transparency in AI",
    "AI and human rights", "legislation on AI surveillance", "data protection at work"
]

# OpenAlex API URL
BASE_URL = "https://api.openalex.org/works"

def search_openalex(keyword):
    """Search OpenAlex for papers related to a keyword."""
    params = {
        "search": keyword,
        "filter": "from_publication_date:2018-01-01",
        "per_page": 100
    }
    response = requests.get(BASE_URL, params=params)
    return response.json() if response.status_code == 200 else None

# Store results
papers = []
co_occurring_keywords = Counter()
authors_counter = Counter()
institutions_counter = Counter()

tqdm.pandas()  # Show progress bar

for keyword in tqdm(keywords, desc="Searching OpenAlex"):
    result = search_openalex(keyword)
    if result and "results" in result:
        for paper in result["results"]:
            papers.append({
                "title": paper.get("title", ""),
                "publication_date": paper.get("publication_date", ""),
                "doi": paper.get("doi", ""),
                "keywords": paper.get("keywords", []),
                "authors": [a["author"].get("display_name", "") for a in paper.get("authorships", [])],
                "institutions": [inst["institution"].get("display_name", "") for a in paper.get("authorships", []) for inst in a.get("institutions", [])]
            })
            # Collect co-occurring keywords
            co_occurring_keywords.update(paper.get("keywords", []))
            # Collect top authors and institutions
            for a in paper.get("authorships", []):
                authors_counter.update([a["author"].get("display_name", "")])
                for inst in a.get("institutions", []):
                    institutions_counter.update([inst["institution"].get("display_name", "")])

# Convert results to DataFrame
papers_df = pd.DataFrame(papers)
co_keywords_df = pd.DataFrame(co_occurring_keywords.items(), columns=["Keyword", "Count"]).sort_values(by="Count", ascending=False)
authors_df = pd.DataFrame(authors_counter.items(), columns=["Author", "Count"]).sort_values(by="Count", ascending=False)
institutions_df = pd.DataFrame(institutions_counter.items(), columns=["Institution", "Count"]).sort_values(by="Count", ascending=False)

# Save to CSV
papers_df.to_csv("openalex_papers.csv", index=False)
co_keywords_df.to_csv("openalex_co_keywords.csv", index=False)
authors_df.to_csv("openalex_top_authors.csv", index=False)
institutions_df.to_csv("openalex_top_institutions.csv", index=False)

print("Data collection complete! CSV files saved.")


##
import pandas as pd

# Load CSVs into DataFrames
papers_df = pd.read_csv("openalex_papers.csv")
co_keywords_df = pd.read_csv("openalex_co_keywords.csv")
authors_df = pd.read_csv("openalex_top_authors.csv")
institutions_df = pd.read_csv("openalex_top_institutions.csv")

# Display first few rows
print("Papers:")
print(papers_df.head())

print("\nTop Co-occurring Keywords:")
print(co_keywords_df.head())

print("\nTop Authors:")
print(authors_df.head())

print("\nTop Institutions:")
print(institutions_df.head())




##


#Check if valid results exist
if 'results' in data and len(data['results']) > 0:
    # Convert JSON data into Pandas DataFrame
    df = pd.DataFrame(data['results'])

    #Process publication year if available
    if 'publication_year' in df.columns:
        df['publication_year'] = pd.to_numeric(df['publication_year'], errors='coerce')  
        df = df.dropna(subset=['publication_year'])  

        #Save data to CSV
        df.to_csv("ai_surveillance_research.csv", index=False)
        print("✅ Data saved as ai_surveillance_research.csv!")

        #Yearly publication count
        yearly_counts = df['publication_year'].value_counts().sort_index()

        #Plot research trends
        plt.figure(figsize=(10, 5))
        plt.plot(yearly_counts.index, yearly_counts.values, marker='o', linestyle='-', color='blue')
        plt.xlabel("Year")
        plt.ylabel("Number of Papers")
        plt.title("Research Trend: AI Surveillance in Workplace (2015-2024)")
        plt.grid()
        plt.show()

        #Histogram using Seaborn
        plt.figure(figsize=(10, 5))
        sns.histplot(df['publication_year'], bins=10, kde=True, color='skyblue')
        plt.title('AI & Surveillance Research Over the Years')
        plt.xlabel('Year')
        plt.ylabel('Number of Publications')
        plt.show()
    else:
        print("⚠️ 'publication_year' column not found in the dataset.")

else:
    print("⚠️ No results found or error in data retrieval.")


# === Text Analysis: Keyword Extraction ===
#Sample abstracts (replace with actual dataset)
abstracts = [
    'Artificial intelligence in workplace surveillance is increasing rapidly.',
    'AI monitoring raises ethical concerns about privacy and data collection.',
    'Machine learning is applied in employee behavior analysis.',
    'Surveillance technologies use deep learning to track employee performance.',
    'AI-driven workplace surveillance impacts mental health and productivity.'
]

#Tokenization & Stopword Removal
stop_words = set(stopwords.words('english'))

def extract_keywords(text):
    words = word_tokenize(text.lower())  
    words = [word for word in words if word.isalnum() and word not in stop_words]  
    return words

# Process abstracts
keywords_list = [extract_keywords(abstract) for abstract in abstracts]

# Flatten and count keywords
all_keywords = [keyword for sublist in keywords_list for keyword in sublist]

#Compute Keyword Co-Occurrences
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([' '.join(keywords) for keywords in keywords_list])
co_occurrence_matrix = (X.T @ X).toarray()

# Convert matrix to DataFrame
keywords = vectorizer.get_feature_names_out()
co_df = pd.DataFrame(co_occurrence_matrix, index=keywords, columns=keywords)

#Filter Co-Occurrences for Key Terms
search_terms = ['artificial', 'intelligence', 'workplace', 'surveillance']
co_occurrence_with_ai = co_df.loc[search_terms].sum(axis=0).sort_values(ascending=False)
co_occurrence_with_ai = co_occurrence_with_ai[co_occurrence_with_ai > 0]

#Plot Co-occurring Keywords
plt.figure(figsize=(10, 7))
plt.bar(co_occurrence_with_ai.index, co_occurrence_with_ai.values, color='skyblue', alpha=0.7)
plt.title('Most Common Keywords Co-occurring with AI Workplace Surveillance', fontsize=14)
plt.xlabel('Keywords', fontsize=12)
plt.ylabel('Co-occurrence Count', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.show()


# === Institutional Analysis ===
if 'authorships' in df.columns:
    all_institutions = []
    
    #Extract institutions safely
    for authorship in df['authorships'].dropna():
        if isinstance(authorship, list):  
            for author in authorship:
                if isinstance(author, dict) and 'institutions' in author:
                    for inst in author['institutions']:
                        if isinstance(inst, dict) and 'display_name' in inst:
                            all_institutions.append(inst['display_name'])

    #Count institutions
    if all_institutions:
        institution_counts = Counter(all_institutions)
        institution_df = pd.DataFrame(institution_counts.items(), columns=['Institution', 'Papers'])    
        institution_df = institution_df.sort_values(by="Papers", ascending=False)

        #Display top 10 institutions
        print(institution_df.head(10))
    else:
        print("⚠️ No institutions found.")
else:
    print("⚠️ 'authorships' column missing in dataset.")


# === Co-Authorship Network Analysis ===
if 'authorships' in df.columns:
    G = nx.Graph()
    
    #Extract co-authorship relationships
    for authorship in df['authorships'].dropna():
        if isinstance(authorship, list):
            authors = [author['author']['display_name'] for author in authorship if 'author' in author and 'display_name' in author['author']]
            for i, author1 in enumerate(authors):
                for author2 in authors[i + 1:]:
                    G.add_edge(author1, author2)
    
    #Plot Co-Authorship Network
    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', font_size=8, node_size=500)
    plt.title("Co-Authorship Network")
    plt.show()
else:
    print("⚠️ No co-authorship data available.")
